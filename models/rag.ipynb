{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass() # 만약 Openai Embedding 미사용시 key 입력 필요 x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass() # langchain hub를 통해 prompt 다운 시 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import DataFrameLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "train = pd.read_csv(\"./train.csv\")\n",
    "test = pd.read_csv('./test.csv')\n",
    "\n",
    "train_data = []\n",
    "\n",
    "for q,a in list(product([f\"질문_{x}\" for x in range(1,3)],[f\"답변_{x}\" for x in range(1,6)])):\n",
    "    for i in range(len(train)):\n",
    "        train_data.append(\n",
    "            \"### 질문: \"+ train.at[i,q] + \"\\n### 답변 : \" + train.at[i,a]\n",
    "        )\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"context\":train_data\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataFrameLoader(df, page_content_column=\"context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 document size가 큰 경우에 적용하면 좋은 splitter 코드 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    "# )\n",
    "# all_splits = text_splitter.split_documents(docs)\n",
    "# 이 부분은 그런 document들이 길면 짜르는 부분인데 저희 train set은 짧기 때문에 안했습니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_splits[1].metadata['start_index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- OpenAI Embedding을 통해 document searching 을 진행합니다.\n",
    "- document를 저장하는 DB 는 [Chroma](https://docs.trychroma.com/getting-started) 입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = OpenAIEmbeddings()\n",
    "# vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings)\n",
    "# retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 만약 embedding 방식이나 DB 호출 방식을 변경하고 싶으면 아래 코드를 참고해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HuggingFace SentTran의 Embedding 사용\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "modelPath = \"distiluse-base-multilingual-cased-v1\"\n",
    "\n",
    "model_kwargs = {'device':'cuda'}\n",
    "\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FAISS 사용\n",
    "\n",
    "# BUILD\n",
    "db = FAISS.from_documents(documents=docs, embedding=embeddings)\n",
    "db.save_local(\"faiss_index\")    # FAISS Index 저장\n",
    "\n",
    "# 아래 코드는 저장된 FAISS를 loading 하는 부분입니다. BUILD를 했다면 BUILD 없이 불러오기만 하면 됩니다\n",
    "db = FAISS.load_local(\"faiss_index\", embeddings)\n",
    "\n",
    "# retriever 정의\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래 `get_seperated_question` 함수는 test의 문장을 문장 마침표를 기준으로 복합질문을 단일 질문으로 변경하는 function입니다.\n",
    "- 아래 함수는 사용하지 않았던 방법이 점수가 높았으나, 문장을 단순하게 출력하게 하고 concat을 한다면 성능이 향상될 가능성은 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seperated_question(q) -> list:\n",
    "    q_list = []\n",
    "    q_count = q.count('?')\n",
    "    d_count = q.count('.')\n",
    "    end_point_cnt = q_count + d_count\n",
    "    if end_point_cnt == 1:\n",
    "        q_list.append(q)\n",
    "    else:\n",
    "        if q_count > d_count:\n",
    "            temp_qs = [(x + '?').strip() for x in q.split('?') if x != '']\n",
    "            q_list.extend(temp_qs)\n",
    "\n",
    "        elif q_count < d_count:\n",
    "            temp_qs = [(x + '.').strip() for x in q.split('.') if x != '']\n",
    "            q_list.extend(temp_qs)\n",
    "\n",
    "        else:\n",
    "            if q.index('.') < q.index('?'): # 질문1. 질문2?\n",
    "                temp_qs = [x.strip() for x in q.split('.')]\n",
    "                temp_qs[0] += '.'\n",
    "                q_list.extend(temp_qs)\n",
    "            else:\n",
    "                temp_qs = [x.strip() for x in q.split('?')]\n",
    "                temp_qs[0] += '?'\n",
    "                q_list.extend(temp_qs)\n",
    "    \n",
    "    return q_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Langchain Prompt 입니다.\n",
    "- Langchain Hub에서 사용한 몇 예시들을 합쳐놓은 겁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "template = \"\"\"You are a kind guide who answers user questions. Your main task is to answer the given question. Here is the instruction when you will provide the answer.\n",
    "1. Use the \"Following Context\" to answer the question. If the answer can find in the context, use the context that exactly matches given answer. If not, use the context as a knowledge document and provide the answer by yourself.\n",
    "2. Provide the most direct and brief answers. Let your answer be not longer than length 300.\n",
    "3. Refrain from adding any supplementary comments, such as apologies or additional explanations.\n",
    "4. Do not reply with recurring sentences.\n",
    "5. Your answer should always be in the same language as the query.\n",
    "\n",
    "Following Context:\n",
    "{context}\n",
    "\n",
    " ### 질문: {question} \n",
    " ### 답변: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'heavytail/kullm-solar-S'\n",
    "\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "\n",
    "device_map = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'CurtisJeon/heavytail-kullm-solar-S-4bit',\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LoRA Adaptor\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    'CurtisJeon/heavytail-kullm-solar-S-lora'\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=300)\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | hf\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "for chunk in rag_chain.stream('당신은 누구입니까?'):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in tqdm(range(len(test)), total=len(test)):\n",
    "    q = test.at[i,'질문']\n",
    "    print('질문:', q)\n",
    "    print('답변:', end=\" \")\n",
    "    for chunk in rag_chain.stream(q):\n",
    "        result.append(chunk)\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    print()\n",
    "    print('------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만약 get_seperated question을 사용한다면\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in tqdm(range(len(test)), total=len(test)):\n",
    "    original_question = test.at[i,'질문']\n",
    "    question_split = get_seperated_question(original_question)\n",
    "    print('원초질문:', original_question)\n",
    "    answers = []\n",
    "    for q in question_split:\n",
    "        print('질문:', q)\n",
    "        print('답변:', end=\" \")\n",
    "        for chunk in rag_chain.stream(q):\n",
    "            answers.append(chunk)\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "        print()\n",
    "            \n",
    "    result.append(answers)\n",
    "    print('------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repetitions(text):\n",
    "    sentences = text.split('. ')\n",
    "    unique_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence not in unique_sentences:\n",
    "            unique_sentences.append(sentence)\n",
    "    return '. '.join(unique_sentences)\n",
    "\n",
    "def cut_to_last_dot(text):\n",
    "    for i in range(len(text)-1, -1, -1):\n",
    "        if text[i] == '.':\n",
    "            break\n",
    "    return text[:i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for r in result:\n",
    "    new_r = \"\\n\".join([cut_to_last_dot(remove_repetitions(x)) for x in r])\n",
    "    preds.append(new_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 데이터셋의 모든 질의에 대한 답변으로부터 512 차원의 Embedding Vector 추출\n",
    "# 평가를 위한 Embedding Vector 추출에 활용하는 모델은 'distiluse-base-multilingual-cased-v1' 이므로 반드시 확인해주세요.\n",
    "from sentence_transformers import SentenceTransformer # SentenceTransformer Version 2.2.2\n",
    "\n",
    "# Embedding Vector 추출에 활용할 모델(distiluse-base-multilingual-cased-v1) 불러오기\n",
    "emb_model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "\n",
    "# 생성한 모든 응답(답변)으로부터 Embedding Vector 추출\n",
    "pred_embeddings = emb_model.encode(preds)\n",
    "pred_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "# 제출 양식 파일(sample_submission.csv)을 활용하여 Embedding Vector로 변환한 결과를 삽입\n",
    "submit.iloc[:,1:] = pred_embeddings\n",
    "# submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./submission_rag.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
